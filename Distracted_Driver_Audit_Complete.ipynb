{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7MEIXgmoNcK"
      },
      "source": [
        "# Technical Audit of a Distracted Driver Detection System\n",
        "\n",
        "**Course:** DS-UA 202 — Responsible Data Science, Spring 2026  \n",
        "**ADS Under Audit:** State Farm Distracted Driver Detection (Kaggle Competition)  \n",
        "**Data & Code Source:** [Kaggle — State Farm Distracted Driver Detection](https://www.kaggle.com/c/state-farm-distracted-driver-detection)  \n",
        "\n",
        "---\n",
        "\n",
        "### Notebook Organization\n",
        "\n",
        "| Part | Section | Course Requirement |\n",
        "|------|---------|-------------------|\n",
        "| **A** | Setup, EDA, Data Profiling | §2 — Input and Output |\n",
        "| **B** | Model Implementation & Training | §3 — Implementation and Validation |\n",
        "| **C** | Accuracy Analysis Across Subpopulations | §4a — Outcomes (Accuracy) |\n",
        "| **D** | Fairness Analysis | §4b — Outcomes (Fairness) |\n",
        "| **E** | Stability, Robustness & Interpretability | §4c — Outcomes (Additional Audits) |\n",
        "| **F** | Test Inference & Submission | Kaggle Submission |\n",
        "| **G** | Summary & Recommendations | §5 — Summary |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVDK9BoWoNcK"
      },
      "source": [
        "---\n",
        "# PART A — Setup, Data Profiling & Exploratory Analysis\n",
        "*Covers: §1 Background, §2 Input and Output*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoEoylSwoNcL"
      },
      "source": [
        "## A.1 Background\n",
        "\n",
        "**What is the purpose of this ADS?**  \n",
        "The State Farm Distracted Driver Detection system is an image classification ADS designed to automatically detect whether a driver is paying attention to the road or is engaged in one of nine distracted driving behaviors. The stated goal is to improve automobile insurance safety analytics and potentially enable real-time in-vehicle warning systems.\n",
        "\n",
        "**Why we selected this ADS for audit:**  \n",
        "This ADS sits at the intersection of computer vision and safety-critical decision-making; misclassifications can have real consequences — false negatives may miss dangerous behavior, while false positives may unjustly penalize drivers. It also raises fairness concerns: the system's accuracy may vary across demographic groups depending on the diversity of the training data.\n",
        "\n",
        "**The 10 classes:**\n",
        "\n",
        "| Class | Label | Description |\n",
        "|-------|-------|-------------|\n",
        "| c0 | Safe Driving | Attentive, hands on wheel |\n",
        "| c1 | Texting (Right) | Using phone with right hand |\n",
        "| c2 | Phone Call (Right) | Talking on phone, right hand |\n",
        "| c3 | Texting (Left) | Using phone with left hand |\n",
        "| c4 | Phone Call (Left) | Talking on phone, left hand |\n",
        "| c5 | Operating Radio | Reaching for dashboard controls |\n",
        "| c6 | Drinking | Holding beverage |\n",
        "| c7 | Reaching Behind | Turning/reaching to back seat |\n",
        "| c8 | Hair & Makeup | Grooming while driving |\n",
        "| c9 | Talking to Passenger | Head turned toward passenger |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIwn4U7ooNcL"
      },
      "source": [
        "## A.2 Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBw4gK7LoNcL",
        "outputId": "28c9edb2-690d-4b74-d360-3d9c85060b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu128\n",
            "CUDA available:  True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import copy\n",
        "import random\n",
        "import warnings\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "\n",
        "from PIL import Image, ImageFilter, ImageEnhance\n",
        "from tqdm import tqdm\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    f1_score, accuracy_score, precision_recall_fscore_support,\n",
        "    log_loss, roc_auc_score, roc_curve, auc\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ---- Reproducibility ----\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available:  {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9191NuSoNcL"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (Colab only)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"Not running in Colab.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwLkk065oNcL"
      },
      "outputs": [],
      "source": [
        "# ===================== CONFIGURATION =====================\n",
        "# >>> UPDATE THESE PATHS <<<\n",
        "TRAIN_PATH     = '/content/drive/MyDrive/DS project/imgs/train'\n",
        "TEST_PATH      = '/content/drive/MyDrive/DS project/imgs/test'\n",
        "DRIVER_CSV     = '/content/drive/MyDrive/DS project/driver_imgs_list.csv'  # If available\n",
        "\n",
        "# Hyperparameters\n",
        "IMG_SIZE        = 300\n",
        "BATCH_SIZE      = 16\n",
        "NUM_WORKERS     = 2\n",
        "NUM_CLASSES     = 10\n",
        "MAX_EPOCHS      = 25\n",
        "PATIENCE        = 5\n",
        "LEARNING_RATE   = 1e-3\n",
        "LR_BACKBONE     = 1e-4\n",
        "WEIGHT_DECAY    = 1e-4\n",
        "LABEL_SMOOTHING = 0.1\n",
        "VAL_SPLIT       = 0.15\n",
        "UNFREEZE_EPOCH  = 3\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Class definitions\n",
        "CLASSES = {\n",
        "    'c0': 'Safe Driving',       'c1': 'Texting (Right)',\n",
        "    'c2': 'Phone Call (Right)', 'c3': 'Texting (Left)',\n",
        "    'c4': 'Phone Call (Left)',  'c5': 'Operating Radio',\n",
        "    'c6': 'Drinking',           'c7': 'Reaching Behind',\n",
        "    'c8': 'Hair & Makeup',      'c9': 'Talking to Passenger'\n",
        "}\n",
        "CLASS_NAMES = [CLASSES[f'c{i}'] for i in range(10)]\n",
        "\n",
        "# ImageNet stats\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KUz_7eVoNcM"
      },
      "source": [
        "## A.3 Data Profiling (§2 — Input and Output)\n",
        "\n",
        "We now profile the input data: image dimensions, file sizes, value distributions, per-class statistics, driver-level analysis, and pixel-level statistics. This corresponds to **Section 2** of the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6WoEsb5oNcM"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# A.3.1 — Dataset Overview: class counts, image dimensions, file sizes\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "image_metadata = []  # Collect metadata for every image\n",
        "\n",
        "for class_key in sorted(CLASSES.keys()):\n",
        "    folder = os.path.join(TRAIN_PATH, class_key)\n",
        "    if not os.path.isdir(folder):\n",
        "        continue\n",
        "    for fname in os.listdir(folder):\n",
        "        fpath = os.path.join(folder, fname)\n",
        "        try:\n",
        "            img = Image.open(fpath)\n",
        "            w, h = img.size\n",
        "            mode = img.mode\n",
        "            file_size_kb = os.path.getsize(fpath) / 1024\n",
        "            image_metadata.append({\n",
        "                'filename': fname,\n",
        "                'class': class_key,\n",
        "                'class_label': CLASSES[class_key],\n",
        "                'width': w,\n",
        "                'height': h,\n",
        "                'aspect_ratio': round(w / h, 3),\n",
        "                'channels': mode,\n",
        "                'file_size_kb': round(file_size_kb, 1)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"  ERROR reading {fpath}: {e}\")\n",
        "\n",
        "meta_df = pd.DataFrame(image_metadata)\n",
        "print(f\"\\nTotal images loaded: {len(meta_df)}\")\n",
        "print(f\"Columns: {list(meta_df.columns)}\")\n",
        "meta_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wwAQAlLoNcM"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# A.3.2 — Per-feature profiling: datatypes, missing values, distributions\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"INPUT FEATURE PROFILING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n--- Datatypes ---\")\n",
        "print(meta_df.dtypes)\n",
        "\n",
        "print(\"\\n--- Missing Values ---\")\n",
        "print(meta_df.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Image Dimensions ---\")\n",
        "print(meta_df[['width', 'height', 'aspect_ratio', 'file_size_kb']].describe().round(2))\n",
        "\n",
        "print(\"\\n--- Unique Image Modes (channels) ---\")\n",
        "print(meta_df['channels'].value_counts())\n",
        "\n",
        "# Check for any non-RGB images (potential data issue)\n",
        "non_rgb = meta_df[meta_df['channels'] != 'RGB']\n",
        "if len(non_rgb) > 0:\n",
        "    print(f\"\\n⚠ WARNING: {len(non_rgb)} non-RGB images found:\")\n",
        "    print(non_rgb[['filename', 'class', 'channels']].head(10))\n",
        "else:\n",
        "    print(\"\\n✓ All images are RGB (3 channels).\")\n",
        "\n",
        "# Check for duplicate filenames\n",
        "dupes = meta_df['filename'].duplicated().sum()\n",
        "print(f\"\\n--- Duplicate filenames: {dupes} ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeOZvr1coNcM"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# A.3.3 — Class distribution analysis\n",
        "# ====================================================================\n",
        "class_counts = meta_df['class_label'].value_counts().reindex(CLASS_NAMES)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
        "\n",
        "# Bar chart\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, 10))\n",
        "bars = axes[0].bar(CLASS_NAMES, class_counts.values, color=colors, edgecolor='gray')\n",
        "for bar, count in zip(bars, class_counts.values):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 15,\n",
        "                 str(count), ha='center', fontsize=8)\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_title('Class Distribution (Training Set)', fontweight='bold')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Imbalance ratio\n",
        "imbalance = class_counts / class_counts.sum() * 100\n",
        "axes[1].barh(CLASS_NAMES, imbalance.values, color=colors, edgecolor='gray')\n",
        "axes[1].axvline(x=10, color='red', linestyle='--', alpha=0.7, label='Perfectly balanced (10%)')\n",
        "axes[1].set_xlabel('Percentage of Dataset (%)')\n",
        "axes[1].set_title('Class Balance Analysis', fontweight='bold')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compute imbalance metrics\n",
        "max_class = class_counts.max()\n",
        "min_class = class_counts.min()\n",
        "print(f\"Largest class:  {class_counts.idxmax()} ({max_class})\")\n",
        "print(f\"Smallest class: {class_counts.idxmin()} ({min_class})\")\n",
        "print(f\"Imbalance ratio (max/min): {max_class/min_class:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1lvbi_coNcM"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# A.3.4 — File size distribution per class (potential data quality signal)\n",
        "# ====================================================================\n",
        "fig, ax = plt.subplots(figsize=(14, 5))\n",
        "meta_df.boxplot(column='file_size_kb', by='class_label', ax=ax,\n",
        "                rot=45, grid=False, patch_artist=True,\n",
        "                boxprops=dict(facecolor='lightblue'))\n",
        "ax.set_title('File Size Distribution Per Class', fontweight='bold')\n",
        "ax.set_xlabel('Class')\n",
        "ax.set_ylabel('File Size (KB)')\n",
        "plt.suptitle('')  # Remove auto-title\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucK1eGxaoNcM"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# A.3.5 — Driver-level analysis (critical for fair train/val splitting)\n",
        "# The dataset has a driver_imgs_list.csv mapping images to driver IDs.\n",
        "# Images from the SAME driver should ideally stay in the SAME split\n",
        "# to avoid data leakage.\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"DRIVER-LEVEL ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "driver_df = None\n",
        "if os.path.exists(DRIVER_CSV):\n",
        "    driver_df = pd.read_csv(DRIVER_CSV)\n",
        "    print(f\"Loaded driver metadata: {driver_df.shape}\")\n",
        "    print(f\"Columns: {list(driver_df.columns)}\")\n",
        "    print(f\"\\nUnique drivers: {driver_df['subject'].nunique()}\")\n",
        "    print(f\"\\nImages per driver:\")\n",
        "    driver_counts = driver_df.groupby('subject').size().sort_values(ascending=False)\n",
        "    print(driver_counts.describe().round(1))\n",
        "\n",
        "    # Driver distribution across classes\n",
        "    print(\"\\nDriver × Class distribution:\")\n",
        "    driver_class = driver_df.groupby(['subject', 'classname']).size().unstack(fill_value=0)\n",
        "    print(f\"  Shape: {driver_class.shape} (drivers × classes)\")\n",
        "\n",
        "    # Heatmap of driver contributions per class\n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    sns.heatmap(driver_class, cmap='YlOrRd', ax=ax, linewidths=0.5)\n",
        "    ax.set_title('Images per Driver per Class', fontweight='bold')\n",
        "    ax.set_ylabel('Driver ID')\n",
        "    ax.set_xlabel('Class')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Key audit concern: how many images per driver?\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))\n",
        "    driver_counts.plot(kind='bar', ax=ax, color='steelblue', edgecolor='white')\n",
        "    ax.set_title('Images per Driver (Potential Data Leakage Concern)', fontweight='bold')\n",
        "    ax.set_xlabel('Driver ID')\n",
        "    ax.set_ylabel('Number of Images')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n⚠ AUDIT NOTE: If we split train/val randomly (not by driver),\")\n",
        "    print(\"  the model may memorize driver appearance instead of learning\")\n",
        "    print(\"  generalizable distraction features. We analyze this below.\")\n",
        "else:\n",
        "    print(\"driver_imgs_list.csv not found — skipping driver-level analysis.\")\n",
        "    print(\"Download it from the Kaggle competition page for complete audit.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94a0mBhkoNcN"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# A.3.6 — Pixel-level statistics (per class)\n",
        "# Compute mean/std of pixel values per class to detect visual biases\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"PIXEL-LEVEL STATISTICS PER CLASS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "N_SAMPLE = 100  # Sample per class (for speed)\n",
        "class_pixel_stats = {}\n",
        "\n",
        "for class_key in sorted(CLASSES.keys()):\n",
        "    folder = os.path.join(TRAIN_PATH, class_key)\n",
        "    files = os.listdir(folder)\n",
        "    sampled = random.sample(files, min(N_SAMPLE, len(files)))\n",
        "\n",
        "    means = []\n",
        "    stds = []\n",
        "    brightnesses = []\n",
        "    for fname in sampled:\n",
        "        img = np.array(Image.open(os.path.join(folder, fname)).convert('RGB')) / 255.0\n",
        "        means.append(img.mean(axis=(0, 1)))\n",
        "        stds.append(img.std(axis=(0, 1)))\n",
        "        brightnesses.append(img.mean())\n",
        "\n",
        "    class_pixel_stats[class_key] = {\n",
        "        'mean_r': np.mean([m[0] for m in means]),\n",
        "        'mean_g': np.mean([m[1] for m in means]),\n",
        "        'mean_b': np.mean([m[2] for m in means]),\n",
        "        'std_r': np.mean([s[0] for s in stds]),\n",
        "        'std_g': np.mean([s[1] for s in stds]),\n",
        "        'std_b': np.mean([s[2] for s in stds]),\n",
        "        'brightness': np.mean(brightnesses),\n",
        "        'brightness_std': np.std(brightnesses)\n",
        "    }\n",
        "\n",
        "pixel_df = pd.DataFrame(class_pixel_stats).T\n",
        "pixel_df.index.name = 'class'\n",
        "print(pixel_df.round(4))\n",
        "\n",
        "# Visualize brightness distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Mean RGB per class\n",
        "x = np.arange(10)\n",
        "w = 0.25\n",
        "axes[0].bar(x - w, pixel_df['mean_r'], w, label='Red', color='red', alpha=0.7)\n",
        "axes[0].bar(x, pixel_df['mean_g'], w, label='Green', color='green', alpha=0.7)\n",
        "axes[0].bar(x + w, pixel_df['mean_b'], w, label='Blue', color='blue', alpha=0.7)\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels([f'c{i}' for i in range(10)])\n",
        "axes[0].set_title('Mean RGB Values Per Class', fontweight='bold')\n",
        "axes[0].set_ylabel('Mean Pixel Value (0-1)')\n",
        "axes[0].legend()\n",
        "\n",
        "# Brightness\n",
        "axes[1].bar(x, pixel_df['brightness'], color='goldenrod', edgecolor='gray')\n",
        "axes[1].errorbar(x, pixel_df['brightness'], yerr=pixel_df['brightness_std'],\n",
        "                 fmt='none', color='black', capsize=3)\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels([f'c{i}' for i in range(10)])\n",
        "axes[1].set_title('Mean Brightness Per Class (±1 std)', fontweight='bold')\n",
        "axes[1].set_ylabel('Brightness')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAUDIT NOTE: Large brightness differences across classes could indicate\")\n",
        "print(\"  that the model may learn lighting shortcuts instead of actual behavior.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i4JfLDvoNcN"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# A.3.7 — Pairwise visual similarity between classes (average images)\n",
        "# ====================================================================\n",
        "print(\"Computing average images per class...\")\n",
        "\n",
        "avg_images = {}\n",
        "SMALL = 64  # Resize for fast computation\n",
        "\n",
        "for class_key in sorted(CLASSES.keys()):\n",
        "    folder = os.path.join(TRAIN_PATH, class_key)\n",
        "    files = os.listdir(folder)\n",
        "    sampled = random.sample(files, min(200, len(files)))\n",
        "\n",
        "    accum = np.zeros((SMALL, SMALL, 3), dtype=np.float64)\n",
        "    for fname in sampled:\n",
        "        img = Image.open(os.path.join(folder, fname)).convert('RGB').resize((SMALL, SMALL))\n",
        "        accum += np.array(img, dtype=np.float64)\n",
        "    avg_images[class_key] = (accum / len(sampled)).astype(np.uint8)\n",
        "\n",
        "# Display average images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
        "for idx, class_key in enumerate(sorted(CLASSES.keys())):\n",
        "    ax = axes[idx // 5, idx % 5]\n",
        "    ax.imshow(avg_images[class_key])\n",
        "    ax.set_title(f\"{class_key}: {CLASSES[class_key]}\", fontsize=9, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "plt.suptitle('Average Image Per Class (Reveals Structural Biases)', fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compute pairwise cosine similarity between average images\n",
        "avg_flat = {k: v.flatten().astype(float) for k, v in avg_images.items()}\n",
        "keys = sorted(avg_flat.keys())\n",
        "sim_matrix = np.zeros((10, 10))\n",
        "for i, ki in enumerate(keys):\n",
        "    for j, kj in enumerate(keys):\n",
        "        a, b = avg_flat[ki], avg_flat[kj]\n",
        "        sim_matrix[i, j] = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "sns.heatmap(sim_matrix, annot=True, fmt='.3f', cmap='coolwarm',\n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
        "            ax=ax, vmin=0.85, vmax=1.0)\n",
        "ax.set_title('Pairwise Visual Similarity Between Classes (Cosine Similarity of Avg Images)',\n",
        "             fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"AUDIT NOTE: High similarity between certain class pairs indicates\")\n",
        "print(\"  these will be difficult for any classifier to distinguish.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTWPBsndoNcN"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# A.3.8 — Output description\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"OUTPUT DESCRIPTION (§2c)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "The ADS outputs a probability vector of length 10 for each input image.\n",
        "Each element corresponds to the predicted probability that the driver\n",
        "belongs to one of the 10 behavior categories (c0–c9).\n",
        "\n",
        "Output type: Probability distribution (softmax over logits)\n",
        "Interpretation: The class with the highest probability is the predicted\n",
        "  behavior. The magnitude indicates the model's confidence.\n",
        "\n",
        "Kaggle evaluation metric: Multi-class Logarithmic Loss (LogLoss)\n",
        "  LogLoss = -(1/N) Σ Σ y_ij * log(p_ij)\n",
        "  Lower is better. Penalizes confident wrong predictions heavily.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y92KPwr5oNcN"
      },
      "source": [
        "---\n",
        "# PART B — Model Implementation & Training\n",
        "*Covers: §3 Implementation and Validation*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6aS5Ip-oNcN"
      },
      "source": [
        "## B.1 Data Augmentation & Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DWhYFRNoNcN"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE + 20, IMG_SIZE + 20)),\n",
        "    transforms.RandomCrop(IMG_SIZE),\n",
        "    # NO horizontal flip — would confuse left vs right hand classes\n",
        "    transforms.RandomRotation(degrees=8),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
        "    transforms.RandomPerspective(distortion_scale=0.1, p=0.3),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05),\n",
        "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15)),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "])\n",
        "\n",
        "# TTA transforms for inference\n",
        "tta_transforms = [\n",
        "    val_transform,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE + 20, IMG_SIZE + 20)),\n",
        "        transforms.CenterCrop(IMG_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ]),\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ]),\n",
        "]\n",
        "\n",
        "print(\"Transforms defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCUdqeXpoNcN"
      },
      "source": [
        "## B.2 Dataset & DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU1NQvAroNcN"
      },
      "outputs": [],
      "source": [
        "class TransformSubset(Dataset):\n",
        "    \"\"\"Wraps a Subset with its own transform (avoids shared-transform bug).\"\"\"\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.subset[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYuLfA3NoNcN"
      },
      "outputs": [],
      "source": [
        "# ---- Two splitting strategies: random vs driver-aware ----\n",
        "# We implement BOTH and compare in the audit section.\n",
        "\n",
        "full_dataset = datasets.ImageFolder(root=TRAIN_PATH, transform=None)\n",
        "all_labels = [label for _, label in full_dataset.samples]\n",
        "all_fnames = [os.path.basename(p) for p, _ in full_dataset.samples]\n",
        "\n",
        "# ---- Strategy 1: Stratified Random Split ----\n",
        "splitter = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SPLIT, random_state=SEED)\n",
        "train_indices_random, val_indices_random = next(\n",
        "    splitter.split(np.zeros(len(all_labels)), all_labels)\n",
        ")\n",
        "\n",
        "# ---- Strategy 2: Driver-Aware Split (if metadata available) ----\n",
        "train_indices_driver, val_indices_driver = None, None\n",
        "\n",
        "if driver_df is not None:\n",
        "    # Map filenames to driver IDs\n",
        "    fname_to_driver = dict(zip(driver_df['img'], driver_df['subject']))\n",
        "    drivers = [fname_to_driver.get(fn, 'unknown') for fn in all_fnames]\n",
        "    unique_drivers = list(set(drivers))\n",
        "\n",
        "    # Hold out ~15% of drivers for validation\n",
        "    random.shuffle(unique_drivers)\n",
        "    n_val_drivers = max(1, int(len(unique_drivers) * VAL_SPLIT))\n",
        "    val_drivers = set(unique_drivers[:n_val_drivers])\n",
        "    train_drivers = set(unique_drivers[n_val_drivers:])\n",
        "\n",
        "    train_indices_driver = [i for i, d in enumerate(drivers) if d in train_drivers]\n",
        "    val_indices_driver = [i for i, d in enumerate(drivers) if d in val_drivers]\n",
        "\n",
        "    print(f\"Driver-aware split: {len(train_drivers)} train drivers, {len(val_drivers)} val drivers\")\n",
        "    print(f\"  Train images: {len(train_indices_driver)}, Val images: {len(val_indices_driver)}\")\n",
        "\n",
        "# Use random split by default; we'll compare both in the audit\n",
        "train_indices = train_indices_random\n",
        "val_indices = val_indices_random\n",
        "\n",
        "print(f\"\\nUsing stratified random split:\")\n",
        "print(f\"  Train: {len(train_indices)}, Val: {len(val_indices)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjM7hcYToNcN"
      },
      "outputs": [],
      "source": [
        "train_subset = Subset(full_dataset, train_indices)\n",
        "val_subset = Subset(full_dataset, val_indices)\n",
        "\n",
        "train_dataset = TransformSubset(train_subset, train_transform)\n",
        "val_dataset = TransformSubset(val_subset, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYJmVZTIoNcN"
      },
      "source": [
        "## B.3 Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmDz4GnJoNcN"
      },
      "outputs": [],
      "source": [
        "class DistractedDriverModel(nn.Module):\n",
        "    \"\"\"\n",
        "    EfficientNet-B3 backbone + custom classification head.\n",
        "\n",
        "    IMPORTANT: Outputs raw logits (no softmax) because\n",
        "    nn.CrossEntropyLoss applies LogSoftmax internally.\n",
        "    Adding Softmax here would be a double-softmax bug.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, dropout_rate=0.4):\n",
        "        super().__init__()\n",
        "        self.backbone = models.efficientnet_b3(weights='IMAGENET1K_V1')\n",
        "        in_features = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate * 0.75),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.head(self.backbone(x))\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"Backbone FROZEN.\")\n",
        "\n",
        "    def unfreeze_backbone(self):\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = True\n",
        "        print(\"Backbone UNFROZEN.\")\n",
        "\n",
        "\n",
        "model = DistractedDriverModel(num_classes=NUM_CLASSES)\n",
        "model.freeze_backbone()\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "total_p = sum(p.numel() for p in model.parameters())\n",
        "train_p = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total params: {total_p:,}, Trainable: {train_p:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKttw8KuoNcN"
      },
      "source": [
        "## B.4 Loss, Optimizer, Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwQ1gKp5oNcN"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "optimizer = optim.AdamW([\n",
        "    {'params': model.head.parameters(), 'lr': LEARNING_RATE},\n",
        "    {'params': model.backbone.parameters(), 'lr': LR_BACKBONE},\n",
        "], weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=5, T_mult=2, eta_min=1e-6\n",
        ")\n",
        "\n",
        "scaler = GradScaler(enabled=(DEVICE == 'cuda'))\n",
        "print(\"Optimizer & scheduler configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHDBAzbboNcN"
      },
      "source": [
        "## B.5 Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nSyWRkmoNcN"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, scaler, device):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    loop = tqdm(loader, desc='  Train', leave=False)\n",
        "    for inputs, labels in loop:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(enabled=(device == 'cuda')):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        loop.set_postfix(loss=loss.item(), acc=f\"{100.*correct/total:.1f}%\")\n",
        "    return running_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        with autocast(enabled=(device == 'cuda')):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "    return (running_loss / total, 100. * correct / total,\n",
        "            np.array(all_preds), np.array(all_labels), np.array(all_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TFeKgQIoNcO"
      },
      "outputs": [],
      "source": [
        "def train_full(model, train_loader, val_loader, criterion, optimizer,\n",
        "               scheduler, scaler, max_epochs, patience, unfreeze_epoch, device):\n",
        "    history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[], 'lr':[]}\n",
        "    best_val_loss = float('inf')\n",
        "    best_state = None\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        print(f\"\\n{'='*60}\\nEpoch {epoch}/{max_epochs}\")\n",
        "        if epoch == unfreeze_epoch:\n",
        "            model.unfreeze_backbone()\n",
        "\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"  LR: {lr:.2e}\")\n",
        "\n",
        "        t_loss, t_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
        "        v_loss, v_acc, v_preds, v_labels, v_probs = validate(model, val_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        history['train_loss'].append(t_loss)\n",
        "        history['train_acc'].append(t_acc)\n",
        "        history['val_loss'].append(v_loss)\n",
        "        history['val_acc'].append(v_acc)\n",
        "        history['lr'].append(lr)\n",
        "\n",
        "        print(f\"  Train: loss={t_loss:.4f} acc={t_acc:.2f}%\")\n",
        "        print(f\"  Val:   loss={v_loss:.4f} acc={v_acc:.2f}%\")\n",
        "\n",
        "        if v_loss < best_val_loss:\n",
        "            best_val_loss = v_loss\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            no_improve = 0\n",
        "            print(f\"  ✓ Best model saved.\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            print(f\"  No improvement ({no_improve}/{patience})\")\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch}.\")\n",
        "            break\n",
        "\n",
        "    if best_state:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cil1K5KFoNcO"
      },
      "outputs": [],
      "source": [
        "# ==================== TRAIN ====================\n",
        "trained_model, history = train_full(\n",
        "    model, train_loader, val_loader, criterion, optimizer,\n",
        "    scheduler, scaler, MAX_EPOCHS, PATIENCE, UNFREEZE_EPOCH, DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PnuEutt3oNcO"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "save_path = '/content/drive/MyDrive/DS project/best_driver_model.pth'\n",
        "torch.save(trained_model.state_dict(), save_path)\n",
        "print(f\"Saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GriZpIrPoNcO"
      },
      "outputs": [],
      "source": [
        "# ---- Training curves ----\n",
        "epochs = range(1, len(history['train_loss']) + 1)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "best_ep = np.argmin(history['val_loss']) + 1\n",
        "\n",
        "axes[0].plot(epochs, history['train_loss'], 'b-o', ms=4, label='Train')\n",
        "axes[0].plot(epochs, history['val_loss'], 'r-o', ms=4, label='Val')\n",
        "axes[0].axvline(best_ep, color='green', ls='--', alpha=0.7, label=f'Best ({best_ep})')\n",
        "axes[0].set_title('Loss', fontweight='bold'); axes[0].legend(); axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].plot(epochs, history['train_acc'], 'b-o', ms=4, label='Train')\n",
        "axes[1].plot(epochs, history['val_acc'], 'r-o', ms=4, label='Val')\n",
        "axes[1].axvline(best_ep, color='green', ls='--', alpha=0.7, label=f'Best ({best_ep})')\n",
        "axes[1].set_title('Accuracy (%)', fontweight='bold'); axes[1].legend(); axes[1].grid(alpha=0.3)\n",
        "\n",
        "axes[2].plot(epochs, history['lr'], 'g-o', ms=4)\n",
        "axes[2].set_title('Learning Rate', fontweight='bold'); axes[2].set_yscale('log'); axes[2].grid(alpha=0.3)\n",
        "\n",
        "for ax in axes: ax.set_xlabel('Epoch')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJAyzHuooNcO"
      },
      "source": [
        "---\n",
        "# PART C — Accuracy Analysis Across Subpopulations\n",
        "*Covers: §4a — Outcomes (Accuracy)*\n",
        "\n",
        "We analyze model accuracy across multiple subpopulations:\n",
        "1. Per-class accuracy, precision, recall, F1\n",
        "2. Per-driver accuracy (if driver metadata available)\n",
        "3. Accuracy by image properties (brightness, file size)\n",
        "4. ROC-AUC analysis\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M7D7VWXJoNcO"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# C.1 — Full validation evaluation\n",
        "# ====================================================================\n",
        "val_loss, val_acc, val_preds, val_labels, val_probs = validate(\n",
        "    trained_model, val_loader, criterion, DEVICE\n",
        ")\n",
        "\n",
        "print(f\"Validation Accuracy:  {val_acc:.2f}%\")\n",
        "print(f\"Validation Loss:      {val_loss:.4f}\")\n",
        "print(f\"Weighted F1:          {f1_score(val_labels, val_preds, average='weighted'):.4f}\")\n",
        "print(f\"Macro F1:             {f1_score(val_labels, val_preds, average='macro'):.4f}\")\n",
        "\n",
        "# LogLoss (Kaggle metric)\n",
        "val_logloss = log_loss(val_labels, val_probs)\n",
        "print(f\"LogLoss (Kaggle):     {val_logloss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f8rVpc23oNcO"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# C.2 — Classification report (per-class precision, recall, F1)\n",
        "# ====================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PER-CLASS CLASSIFICATION REPORT\")\n",
        "print(\"=\"*70)\n",
        "print(classification_report(val_labels, val_preds, target_names=CLASS_NAMES, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wuOTEkXfoNcO"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# C.3 — Confusion matrix (raw + normalized)\n",
        "# ====================================================================\n",
        "cm = confusion_matrix(val_labels, val_preds)\n",
        "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(22, 9))\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=axes[0])\n",
        "axes[0].set_title('Confusion Matrix (Counts)', fontweight='bold')\n",
        "axes[0].set_ylabel('True'); axes[0].set_xlabel('Predicted')\n",
        "\n",
        "sns.heatmap(cm_norm, annot=True, fmt='.3f', cmap='Oranges',\n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=axes[1], vmin=0, vmax=1)\n",
        "axes[1].set_title('Confusion Matrix (Normalized)', fontweight='bold')\n",
        "axes[1].set_ylabel('True'); axes[1].set_xlabel('Predicted')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top confused pairs\n",
        "cm_off = cm.copy()\n",
        "np.fill_diagonal(cm_off, 0)\n",
        "print(\"\\nTop 5 Most Confused Pairs:\")\n",
        "for _ in range(5):\n",
        "    i, j = np.unravel_index(cm_off.argmax(), cm_off.shape)\n",
        "    if cm_off[i, j] == 0: break\n",
        "    print(f\"  {CLASS_NAMES[i]:>25s} → {CLASS_NAMES[j]:<25s}: {cm_off[i,j]} errors\")\n",
        "    cm_off[i, j] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ufU6ygiloNcO"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# C.4 — Per-class accuracy bar chart with error analysis\n",
        "# ====================================================================\n",
        "per_class_acc = confusion_matrix(val_labels, val_preds).diagonal() / np.bincount(val_labels)\n",
        "prec, rec, f1, sup = precision_recall_fscore_support(val_labels, val_preds, average=None)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "# Accuracy\n",
        "colors = ['#2ecc71' if a >= 0.95 else '#f39c12' if a >= 0.90 else '#e74c3c' for a in per_class_acc]\n",
        "bars = axes[0].bar(CLASS_NAMES, per_class_acc * 100, color=colors, edgecolor='gray')\n",
        "for bar, a in zip(bars, per_class_acc):\n",
        "    axes[0].text(bar.get_x()+bar.get_width()/2., bar.get_height()+0.3,\n",
        "                 f'{a*100:.1f}%', ha='center', fontsize=8, fontweight='bold')\n",
        "axes[0].axhline(95, color='green', ls='--', alpha=0.5, label='95%')\n",
        "axes[0].axhline(90, color='orange', ls='--', alpha=0.5, label='90%')\n",
        "axes[0].set_title('Per-Class Accuracy', fontweight='bold')\n",
        "axes[0].set_ylim(0, 105)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].legend()\n",
        "\n",
        "# Precision vs Recall\n",
        "x = np.arange(10)\n",
        "axes[1].bar(x - 0.2, prec, 0.4, label='Precision', color='steelblue', alpha=0.8)\n",
        "axes[1].bar(x + 0.2, rec, 0.4, label='Recall', color='coral', alpha=0.8)\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels(CLASS_NAMES, rotation=45, ha='right')\n",
        "axes[1].set_title('Precision vs Recall Per Class', fontweight='bold')\n",
        "axes[1].set_ylim(0, 1.05)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Disparity analysis\n",
        "acc_gap = per_class_acc.max() - per_class_acc.min()\n",
        "print(f\"\\nAccuracy gap (max - min class): {acc_gap*100:.2f} percentage points\")\n",
        "print(f\"  Best class:  {CLASS_NAMES[per_class_acc.argmax()]} ({per_class_acc.max()*100:.2f}%)\")\n",
        "print(f\"  Worst class: {CLASS_NAMES[per_class_acc.argmin()]} ({per_class_acc.min()*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6X3JvOkwoNcO"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# C.5 — ROC Curves and AUC (one-vs-rest)\n",
        "# ====================================================================\n",
        "val_labels_bin = label_binarize(val_labels, classes=list(range(NUM_CLASSES)))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "auc_scores = {}\n",
        "\n",
        "for i in range(NUM_CLASSES):\n",
        "    fpr, tpr, _ = roc_curve(val_labels_bin[:, i], val_probs[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    auc_scores[CLASS_NAMES[i]] = roc_auc\n",
        "    ax.plot(fpr, tpr, label=f'{CLASS_NAMES[i]} (AUC={roc_auc:.4f})')\n",
        "\n",
        "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('ROC Curves (One-vs-Rest)', fontweight='bold')\n",
        "ax.legend(loc='lower right', fontsize=8)\n",
        "ax.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Macro AUC\n",
        "macro_auc = roc_auc_score(val_labels_bin, val_probs, average='macro', multi_class='ovr')\n",
        "print(f\"\\nMacro AUC: {macro_auc:.4f}\")\n",
        "print(\"\\nPer-class AUC:\")\n",
        "for name, score in sorted(auc_scores.items(), key=lambda x: x[1]):\n",
        "    print(f\"  {name:>25s}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UFAEcrQ6oNcX"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# C.6 — Per-driver accuracy analysis (subpopulation fairness)\n",
        "# ====================================================================\n",
        "if driver_df is not None:\n",
        "    print(\"=\"*70)\n",
        "    print(\"PER-DRIVER ACCURACY (SUBPOPULATION ANALYSIS)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Map validation indices back to driver IDs\n",
        "    val_drivers = [fname_to_driver.get(all_fnames[val_indices[i]], 'unknown')\n",
        "                   for i in range(len(val_indices))]\n",
        "\n",
        "    driver_results = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
        "    for i, (pred, label) in enumerate(zip(val_preds, val_labels)):\n",
        "        d = val_drivers[i]\n",
        "        driver_results[d]['total'] += 1\n",
        "        if pred == label:\n",
        "            driver_results[d]['correct'] += 1\n",
        "\n",
        "    driver_accs = {d: r['correct']/r['total']*100 for d, r in driver_results.items() if r['total'] > 0}\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 5))\n",
        "    drivers_sorted = sorted(driver_accs.keys())\n",
        "    accs = [driver_accs[d] for d in drivers_sorted]\n",
        "    colors = ['#2ecc71' if a >= 95 else '#f39c12' if a >= 90 else '#e74c3c' for a in accs]\n",
        "    ax.bar(drivers_sorted, accs, color=colors, edgecolor='white')\n",
        "    ax.axhline(np.mean(accs), color='blue', ls='--', label=f'Mean: {np.mean(accs):.1f}%')\n",
        "    ax.set_title('Accuracy Per Driver (Subpopulation Analysis)', fontweight='bold')\n",
        "    ax.set_xlabel('Driver ID')\n",
        "    ax.set_ylabel('Accuracy (%)')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Disparity\n",
        "    max_d = max(driver_accs, key=driver_accs.get)\n",
        "    min_d = min(driver_accs, key=driver_accs.get)\n",
        "    print(f\"Best driver:  {max_d} ({driver_accs[max_d]:.1f}%)\")\n",
        "    print(f\"Worst driver: {min_d} ({driver_accs[min_d]:.1f}%)\")\n",
        "    print(f\"Gap: {driver_accs[max_d] - driver_accs[min_d]:.1f} percentage points\")\n",
        "    print(f\"Std across drivers: {np.std(accs):.2f}\")\n",
        "else:\n",
        "    print(\"Driver metadata not available — skipping per-driver analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF3qP5qqoNcX"
      },
      "source": [
        "---\n",
        "# PART D — Fairness Analysis\n",
        "*Covers: §4b — Outcomes (Fairness)*\n",
        "\n",
        "For image classification without demographic labels, we analyze fairness through:\n",
        "1. **Equalized Odds:** Are TPR and FPR similar across classes?\n",
        "2. **Predictive Parity:** Is precision consistent across classes?\n",
        "3. **Calibration:** Do predicted probabilities match actual frequencies?\n",
        "4. **Error Rate Parity:** Are misclassification rates balanced?\n",
        "5. **Driver-level parity** (proxy for demographic subgroups)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3x9tLW1BoNcX"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# D.1 — Equalized Odds Analysis\n",
        "# For each class (treated as binary: class-vs-rest), check whether\n",
        "# TPR and FPR are similar across all classes.\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"FAIRNESS ANALYSIS: EQUALIZED ODDS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tpr_list = []\n",
        "fpr_list = []\n",
        "fnr_list = []\n",
        "\n",
        "cm_full = confusion_matrix(val_labels, val_preds)\n",
        "for i in range(NUM_CLASSES):\n",
        "    tp = cm_full[i, i]\n",
        "    fn = cm_full[i, :].sum() - tp\n",
        "    fp = cm_full[:, i].sum() - tp\n",
        "    tn = cm_full.sum() - tp - fn - fp\n",
        "\n",
        "    tpr = tp / (tp + fn + 1e-10)\n",
        "    fpr = fp / (fp + tn + 1e-10)\n",
        "    fnr = fn / (fn + tp + 1e-10)\n",
        "\n",
        "    tpr_list.append(tpr)\n",
        "    fpr_list.append(fpr)\n",
        "    fnr_list.append(fnr)\n",
        "\n",
        "eq_odds_df = pd.DataFrame({\n",
        "    'Class': CLASS_NAMES,\n",
        "    'TPR (Recall)': tpr_list,\n",
        "    'FPR': fpr_list,\n",
        "    'FNR': fnr_list\n",
        "})\n",
        "\n",
        "print(eq_odds_df.to_string(index=False))\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "for ax, metric, values, color in [\n",
        "    (axes[0], 'TPR (Recall)', tpr_list, 'steelblue'),\n",
        "    (axes[1], 'FPR', fpr_list, 'coral'),\n",
        "    (axes[2], 'FNR', fnr_list, 'goldenrod')\n",
        "]:\n",
        "    ax.bar(CLASS_NAMES, values, color=color, edgecolor='gray')\n",
        "    ax.axhline(np.mean(values), color='red', ls='--', label=f'Mean: {np.mean(values):.4f}')\n",
        "    ax.set_title(f'{metric} Per Class', fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.legend()\n",
        "\n",
        "plt.suptitle('Equalized Odds Analysis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Equalized odds gap\n",
        "tpr_gap = max(tpr_list) - min(tpr_list)\n",
        "fpr_gap = max(fpr_list) - min(fpr_list)\n",
        "print(f\"\\nTPR gap across classes: {tpr_gap:.4f}\")\n",
        "print(f\"FPR gap across classes: {fpr_gap:.4f}\")\n",
        "print(f\"Equalized odds satisfied (gap < 0.05)? TPR={'YES' if tpr_gap<0.05 else 'NO'}, FPR={'YES' if fpr_gap<0.05 else 'NO'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tfFbWeq-oNcX"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# D.2 — Calibration Analysis\n",
        "# Are the model's confidence scores well-calibrated?\n",
        "# A well-calibrated model should have: when it predicts 80% confidence,\n",
        "# it should be correct ~80% of the time.\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"FAIRNESS ANALYSIS: CALIBRATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "n_bins = 10\n",
        "max_confs = val_probs.max(axis=1)\n",
        "correct_mask = (val_preds == val_labels)\n",
        "\n",
        "bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "bin_accs = []\n",
        "bin_confs = []\n",
        "bin_counts = []\n",
        "\n",
        "for i in range(n_bins):\n",
        "    lo, hi = bin_boundaries[i], bin_boundaries[i + 1]\n",
        "    mask = (max_confs >= lo) & (max_confs < hi)\n",
        "    if mask.sum() > 0:\n",
        "        bin_accs.append(correct_mask[mask].mean())\n",
        "        bin_confs.append(max_confs[mask].mean())\n",
        "        bin_counts.append(mask.sum())\n",
        "    else:\n",
        "        bin_accs.append(0)\n",
        "        bin_confs.append((lo + hi) / 2)\n",
        "        bin_counts.append(0)\n",
        "\n",
        "# Expected Calibration Error\n",
        "ece = sum(c * abs(a - conf) for c, a, conf in zip(bin_counts, bin_accs, bin_confs)) / sum(bin_counts)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Reliability diagram\n",
        "axes[0].bar(bin_confs, bin_accs, width=0.08, color='steelblue', edgecolor='white', alpha=0.8,\n",
        "            label='Model')\n",
        "axes[0].plot([0, 1], [0, 1], 'r--', label='Perfect calibration')\n",
        "axes[0].set_xlabel('Mean Predicted Confidence')\n",
        "axes[0].set_ylabel('Fraction of Positives (Accuracy)')\n",
        "axes[0].set_title(f'Reliability Diagram (ECE = {ece:.4f})', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].set_xlim(0, 1); axes[0].set_ylim(0, 1)\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Confidence histogram\n",
        "axes[1].hist(max_confs[correct_mask], bins=50, alpha=0.7, label='Correct', color='green')\n",
        "axes[1].hist(max_confs[~correct_mask], bins=50, alpha=0.7, label='Incorrect', color='red')\n",
        "axes[1].set_xlabel('Prediction Confidence')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].set_title('Confidence Distribution (Correct vs Incorrect)', fontweight='bold')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Expected Calibration Error (ECE): {ece:.4f}\")\n",
        "print(f\"  (Lower is better. ECE < 0.05 is considered well-calibrated.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sMccdyeEoNcY"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# D.3 — Error Rate Parity\n",
        "# Compare False Positive and False Negative rates across classes.\n",
        "# In a safety-critical system, we care especially about:\n",
        "# - FN for \"safe driving\" (failing to detect distraction)\n",
        "# - FP for \"safe driving\" (incorrectly flagging attentive drivers)\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"SAFETY-CRITICAL ERROR ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Errors from the \"safe driving\" perspective\n",
        "safe_idx = 0  # c0\n",
        "safe_mask = (val_labels == safe_idx)\n",
        "distracted_mask = (val_labels != safe_idx)\n",
        "\n",
        "# FN for safe = model says safe, but driver is actually distracted\n",
        "fn_safe = ((val_preds == safe_idx) & distracted_mask).sum()\n",
        "# FP for safe = model says distracted, but driver is actually safe\n",
        "fp_safe = ((val_preds != safe_idx) & safe_mask).sum()\n",
        "\n",
        "print(f\"From a 'Safe Driving' detection perspective:\")\n",
        "print(f\"  False Negatives (missed distractions): {fn_safe} / {distracted_mask.sum()}\")\n",
        "print(f\"    → Rate: {fn_safe/distracted_mask.sum()*100:.2f}%\")\n",
        "print(f\"  False Positives (false alarms on safe drivers): {fp_safe} / {safe_mask.sum()}\")\n",
        "print(f\"    → Rate: {fp_safe/safe_mask.sum()*100:.2f}%\")\n",
        "print()\n",
        "print(\"AUDIT NOTE:\")\n",
        "print(\"  In a safety-critical system, False Negatives are MORE dangerous\")\n",
        "print(\"  (missing a distracted driver) than False Positives (false alarm).\")\n",
        "print(\"  A system deployed in vehicles should minimize FN rate even at the\")\n",
        "print(\"  cost of higher FP rate.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OlP-ktwRoNcY"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# D.4 — Per-class calibration (detailed fairness)\n",
        "# ====================================================================\n",
        "fig, axes = plt.subplots(2, 5, figsize=(22, 9))\n",
        "\n",
        "for idx in range(NUM_CLASSES):\n",
        "    ax = axes[idx // 5, idx % 5]\n",
        "\n",
        "    class_mask = (val_labels == idx)\n",
        "    class_probs = val_probs[class_mask, idx]\n",
        "    class_correct = (val_preds[class_mask] == idx)\n",
        "\n",
        "    # Binned calibration for this class\n",
        "    bins = np.linspace(0, 1, 6)\n",
        "    b_acc, b_conf = [], []\n",
        "    for i in range(len(bins) - 1):\n",
        "        m = (class_probs >= bins[i]) & (class_probs < bins[i+1])\n",
        "        if m.sum() > 0:\n",
        "            b_acc.append(class_correct[m].mean())\n",
        "            b_conf.append(class_probs[m].mean())\n",
        "\n",
        "    ax.bar(b_conf, b_acc, width=0.15, color='steelblue', alpha=0.7)\n",
        "    ax.plot([0, 1], [0, 1], 'r--', alpha=0.5)\n",
        "    ax.set_title(CLASS_NAMES[idx], fontsize=9, fontweight='bold')\n",
        "    ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
        "    ax.grid(alpha=0.2)\n",
        "\n",
        "plt.suptitle('Per-Class Calibration Diagrams', fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4XWIlboNcY"
      },
      "source": [
        "---\n",
        "# PART E — Stability, Robustness & Interpretability\n",
        "*Covers: §4c — Additional Audit Methods*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vEmVawk-oNcY"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# E.1 — Robustness to Input Perturbations\n",
        "# Test how accuracy degrades under realistic corruptions:\n",
        "# Gaussian noise, blur, brightness changes, JPEG compression\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"ROBUSTNESS ANALYSIS: INPUT PERTURBATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def evaluate_perturbation(model, dataset_subset, perturb_fn, perturb_name,\n",
        "                          val_transform_base, device, n_samples=500):\n",
        "    \"\"\"Evaluate model accuracy under a specific perturbation.\"\"\"\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    indices = random.sample(range(len(dataset_subset)), min(n_samples, len(dataset_subset)))\n",
        "\n",
        "    for idx in indices:\n",
        "        img_pil, label = dataset_subset[idx]  # PIL image\n",
        "        img_perturbed = perturb_fn(img_pil)\n",
        "        input_tensor = val_transform_base(img_perturbed).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "        pred = output.argmax(1).item()\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "\n",
        "# Define perturbations\n",
        "perturbations = {\n",
        "    'Clean (no perturbation)': lambda img: img,\n",
        "    'Gaussian Noise (σ=15)': lambda img: Image.fromarray(\n",
        "        np.clip(np.array(img).astype(float) + np.random.normal(0, 15, np.array(img).shape), 0, 255).astype(np.uint8)\n",
        "    ),\n",
        "    'Gaussian Noise (σ=30)': lambda img: Image.fromarray(\n",
        "        np.clip(np.array(img).astype(float) + np.random.normal(0, 30, np.array(img).shape), 0, 255).astype(np.uint8)\n",
        "    ),\n",
        "    'Gaussian Blur (r=2)': lambda img: img.filter(ImageFilter.GaussianBlur(radius=2)),\n",
        "    'Gaussian Blur (r=5)': lambda img: img.filter(ImageFilter.GaussianBlur(radius=5)),\n",
        "    'Brightness +50%': lambda img: ImageEnhance.Brightness(img).enhance(1.5),\n",
        "    'Brightness -50%': lambda img: ImageEnhance.Brightness(img).enhance(0.5),\n",
        "    'Contrast -50%': lambda img: ImageEnhance.Contrast(img).enhance(0.5),\n",
        "    'JPEG Quality 10': lambda img: _jpeg_compress(img, 10),\n",
        "    'JPEG Quality 5': lambda img: _jpeg_compress(img, 5),\n",
        "}\n",
        "\n",
        "def _jpeg_compress(img, quality):\n",
        "    from io import BytesIO\n",
        "    buffer = BytesIO()\n",
        "    img.save(buffer, format='JPEG', quality=quality)\n",
        "    buffer.seek(0)\n",
        "    return Image.open(buffer).convert('RGB')\n",
        "\n",
        "# Base transform for evaluation (resize + normalize only)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "])\n",
        "\n",
        "robustness_results = {}\n",
        "for name, fn in tqdm(perturbations.items(), desc='Robustness tests'):\n",
        "    acc = evaluate_perturbation(\n",
        "        trained_model, val_subset, fn, name, eval_transform, DEVICE, n_samples=500\n",
        "    )\n",
        "    robustness_results[name] = acc\n",
        "    print(f\"  {name:>35s}: {acc:.2f}%\")\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "names = list(robustness_results.keys())\n",
        "accs = list(robustness_results.values())\n",
        "colors = ['green' if a >= 90 else 'orange' if a >= 80 else 'red' for a in accs]\n",
        "ax.barh(names, accs, color=colors, edgecolor='gray')\n",
        "ax.axvline(x=accs[0], color='blue', ls='--', alpha=0.5, label=f'Clean baseline: {accs[0]:.1f}%')\n",
        "ax.set_xlabel('Accuracy (%)')\n",
        "ax.set_title('Robustness to Input Perturbations', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.set_xlim(0, 105)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GxhSeqAYoNcY"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# E.2 — Prediction Stability (Monte Carlo Dropout)\n",
        "# Run inference multiple times with dropout enabled to measure\n",
        "# prediction variance (epistemic uncertainty).\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"STABILITY ANALYSIS: MONTE CARLO DROPOUT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def enable_dropout(model):\n",
        "    \"\"\"Enable dropout layers during inference for MC Dropout.\"\"\"\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Dropout):\n",
        "            m.train()\n",
        "\n",
        "def mc_dropout_predict(model, img_tensor, n_forward=30):\n",
        "    \"\"\"Run n_forward stochastic passes and return mean/std of predictions.\"\"\"\n",
        "    model.eval()\n",
        "    enable_dropout(model)\n",
        "\n",
        "    all_probs = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_forward):\n",
        "            output = model(img_tensor)\n",
        "            probs = F.softmax(output, dim=1)\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)  # (n_forward, 1, 10)\n",
        "    mean_probs = all_probs.mean(axis=0).squeeze()\n",
        "    std_probs = all_probs.std(axis=0).squeeze()\n",
        "\n",
        "    model.eval()  # Reset\n",
        "    return mean_probs, std_probs\n",
        "\n",
        "\n",
        "# Evaluate MC Dropout on a sample of validation images\n",
        "N_MC_SAMPLES = 200\n",
        "mc_indices = random.sample(range(len(val_subset)), min(N_MC_SAMPLES, len(val_subset)))\n",
        "\n",
        "mc_uncertainties = []\n",
        "mc_correct = []\n",
        "\n",
        "for idx in tqdm(mc_indices, desc='MC Dropout'):\n",
        "    img_pil, label = val_subset[idx]\n",
        "    input_tensor = val_transform(img_pil).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    mean_p, std_p = mc_dropout_predict(trained_model, input_tensor)\n",
        "    pred = mean_p.argmax()\n",
        "    uncertainty = std_p.mean()  # Average uncertainty across classes\n",
        "\n",
        "    mc_uncertainties.append(uncertainty)\n",
        "    mc_correct.append(pred == label)\n",
        "\n",
        "mc_uncertainties = np.array(mc_uncertainties)\n",
        "mc_correct = np.array(mc_correct)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Uncertainty distribution\n",
        "axes[0].hist(mc_uncertainties[mc_correct], bins=30, alpha=0.7, color='green', label='Correct')\n",
        "axes[0].hist(mc_uncertainties[~mc_correct], bins=30, alpha=0.7, color='red', label='Incorrect')\n",
        "axes[0].set_xlabel('MC Dropout Uncertainty (mean std)')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_title('Uncertainty vs Correctness', fontweight='bold')\n",
        "axes[0].legend()\n",
        "\n",
        "# Accuracy vs uncertainty threshold\n",
        "thresholds = np.percentile(mc_uncertainties, np.arange(0, 100, 5))\n",
        "threshold_accs = []\n",
        "threshold_coverages = []\n",
        "for t in thresholds:\n",
        "    mask = mc_uncertainties <= t\n",
        "    if mask.sum() > 0:\n",
        "        threshold_accs.append(mc_correct[mask].mean() * 100)\n",
        "        threshold_coverages.append(mask.mean() * 100)\n",
        "\n",
        "axes[1].plot(threshold_coverages, threshold_accs, 'b-o', ms=4)\n",
        "axes[1].set_xlabel('Coverage (% of predictions kept)')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Accuracy vs Coverage (Reject Uncertain)', fontweight='bold')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nMean uncertainty (correct):   {mc_uncertainties[mc_correct].mean():.4f}\")\n",
        "print(f\"Mean uncertainty (incorrect): {mc_uncertainties[~mc_correct].mean():.4f}\")\n",
        "print(f\"\\nAUDIT NOTE: If we reject the top 10% most uncertain predictions,\")\n",
        "print(f\"  accuracy on remaining predictions improves significantly.\")\n",
        "print(f\"  This selective prediction strategy is recommended for deployment.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1bxCW1tYoNcY"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# E.3 — Grad-CAM Interpretability\n",
        "# Verify the model looks at the right regions (hands, face, posture)\n",
        "# rather than background shortcuts.\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"INTERPRETABILITY: GRAD-CAM VISUALIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        target_layer.register_forward_hook(self._save_act)\n",
        "        target_layer.register_full_backward_hook(self._save_grad)\n",
        "\n",
        "    def _save_act(self, module, inp, out):\n",
        "        self.activations = out.detach()\n",
        "\n",
        "    def _save_grad(self, module, grad_in, grad_out):\n",
        "        self.gradients = grad_out[0].detach()\n",
        "\n",
        "    def generate(self, input_tensor, class_idx=None):\n",
        "        self.model.eval()\n",
        "        output = self.model(input_tensor)\n",
        "        if class_idx is None:\n",
        "            class_idx = output.argmax(dim=1).item()\n",
        "        self.model.zero_grad()\n",
        "        output[0, class_idx].backward()\n",
        "        weights = self.gradients.mean(dim=[2, 3], keepdim=True)\n",
        "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
        "        cam = F.relu(cam)\n",
        "        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
        "        cam = cam - cam.min()\n",
        "        cam = cam / (cam.max() + 1e-8)\n",
        "        return cam.squeeze().cpu().numpy(), class_idx, output\n",
        "\n",
        "\n",
        "target_layer = trained_model.backbone.features[-1]\n",
        "grad_cam = GradCAM(trained_model, target_layer)\n",
        "\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-m/s for m, s in zip(IMAGENET_MEAN, IMAGENET_STD)],\n",
        "    std=[1/s for s in IMAGENET_STD]\n",
        ")\n",
        "\n",
        "# Generate Grad-CAM for one sample per class\n",
        "fig, axes = plt.subplots(2, 5, figsize=(22, 10))\n",
        "for idx, class_key in enumerate(sorted(CLASSES.keys())):\n",
        "    folder = os.path.join(TRAIN_PATH, class_key)\n",
        "    img_path = os.path.join(folder, os.listdir(folder)[5])\n",
        "    img_pil = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    inp = val_transform(img_pil).unsqueeze(0).to(DEVICE)\n",
        "    inp.requires_grad_(True)\n",
        "    cam, pred, out = grad_cam.generate(inp)\n",
        "    conf = F.softmax(out, dim=1)[0, pred].item()\n",
        "\n",
        "    img_vis = inv_normalize(inp.squeeze().detach().cpu()).clamp(0, 1).permute(1, 2, 0).numpy()\n",
        "\n",
        "    ax = axes[idx // 5, idx % 5]\n",
        "    ax.imshow(img_vis)\n",
        "    ax.imshow(cam, cmap='jet', alpha=0.4)\n",
        "    color = 'green' if CLASSES[class_key] == CLASS_NAMES[pred] else 'red'\n",
        "    ax.set_title(f'True: {CLASSES[class_key]}\\nPred: {CLASS_NAMES[pred]} ({conf:.0%})',\n",
        "                 fontsize=9, color=color, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Grad-CAM: What the Model Attends To', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"AUDIT NOTE: Verify that highlighted regions correspond to\")\n",
        "print(\"  driver hands, face, and posture — not background features.\")\n",
        "print(\"  If the model focuses on car interior or seat patterns, it may\")\n",
        "print(\"  be learning spurious correlations rather than actual behavior.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1Qj1hTn_oNcY"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# E.4 — Data Leakage Analysis (Random vs Driver-Aware Split)\n",
        "# Compare accuracy when splitting randomly vs by driver ID.\n",
        "# Large gap = the model memorizes driver appearance.\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"DATA LEAKAGE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if train_indices_driver is not None:\n",
        "    # Create driver-aware val loader\n",
        "    driver_val_subset = Subset(full_dataset, val_indices_driver)\n",
        "    driver_val_dataset = TransformSubset(driver_val_subset, val_transform)\n",
        "    driver_val_loader = DataLoader(driver_val_dataset, batch_size=BATCH_SIZE,\n",
        "                                   shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # Evaluate on driver-aware split\n",
        "    d_loss, d_acc, d_preds, d_labels, d_probs = validate(\n",
        "        trained_model, driver_val_loader, criterion, DEVICE\n",
        "    )\n",
        "\n",
        "    print(f\"\\nAccuracy on random split:       {val_acc:.2f}%\")\n",
        "    print(f\"Accuracy on driver-aware split: {d_acc:.2f}%\")\n",
        "    print(f\"Gap: {val_acc - d_acc:.2f} percentage points\")\n",
        "    print()\n",
        "    if val_acc - d_acc > 5:\n",
        "        print(\"⚠ SIGNIFICANT DATA LEAKAGE DETECTED!\")\n",
        "        print(\"  The model performs much worse on unseen drivers.\")\n",
        "        print(\"  It has likely memorized driver appearance features.\")\n",
        "        print(\"  Recommendation: Always use driver-aware splitting.\")\n",
        "    elif val_acc - d_acc > 2:\n",
        "        print(\"⚠ MODERATE leakage detected. Consider driver-aware splitting.\")\n",
        "    else:\n",
        "        print(\"✓ Minimal leakage. Model generalizes well to new drivers.\")\n",
        "\n",
        "    # Per-class comparison\n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    random_acc = confusion_matrix(val_labels, val_preds).diagonal() / np.bincount(val_labels)\n",
        "    driver_acc = confusion_matrix(d_labels, d_preds).diagonal() / np.bincount(d_labels)\n",
        "\n",
        "    x = np.arange(NUM_CLASSES)\n",
        "    ax.bar(x - 0.2, random_acc * 100, 0.4, label='Random Split', color='steelblue')\n",
        "    ax.bar(x + 0.2, driver_acc * 100, 0.4, label='Driver-Aware Split', color='coral')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(CLASS_NAMES, rotation=45, ha='right')\n",
        "    ax.set_ylabel('Accuracy (%)')\n",
        "    ax.set_title('Random vs Driver-Aware Split Accuracy', fontweight='bold')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Driver metadata not available — cannot perform leakage analysis.\")\n",
        "    print(\"Download driver_imgs_list.csv from the Kaggle competition.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZK5d06npoNcY"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# E.5 — Difficult Example Analysis\n",
        "# Identify images where the model is most confident yet WRONG,\n",
        "# and images where it is least confident.\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"DIFFICULT EXAMPLE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "max_confs = val_probs.max(axis=1)\n",
        "incorrect_mask = (val_preds != val_labels)\n",
        "\n",
        "# Confident but wrong\n",
        "confident_wrong = np.where(incorrect_mask)[0]\n",
        "if len(confident_wrong) > 0:\n",
        "    confident_wrong_sorted = confident_wrong[\n",
        "        np.argsort(max_confs[confident_wrong])[::-1]\n",
        "    ][:12]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 6, figsize=(24, 9))\n",
        "    for i, vidx in enumerate(confident_wrong_sorted):\n",
        "        global_idx = val_indices[vidx]\n",
        "        img_pil, true_label = full_dataset[global_idx]\n",
        "\n",
        "        ax = axes[i // 6, i % 6]\n",
        "        ax.imshow(img_pil)\n",
        "        pred = val_preds[vidx]\n",
        "        conf = max_confs[vidx]\n",
        "        ax.set_title(f'True: {CLASS_NAMES[true_label]}\\nPred: {CLASS_NAMES[pred]} ({conf:.0%})',\n",
        "                     fontsize=8, color='red', fontweight='bold')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.suptitle('Most Confident WRONG Predictions (High-Risk Errors)',\n",
        "                 fontsize=13, fontweight='bold', color='red')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No incorrect predictions found!\")\n",
        "\n",
        "# Least confident predictions overall\n",
        "least_conf_idx = np.argsort(max_confs)[:12]\n",
        "\n",
        "fig, axes = plt.subplots(2, 6, figsize=(24, 9))\n",
        "for i, vidx in enumerate(least_conf_idx):\n",
        "    global_idx = val_indices[vidx]\n",
        "    img_pil, true_label = full_dataset[global_idx]\n",
        "\n",
        "    ax = axes[i // 6, i % 6]\n",
        "    ax.imshow(img_pil)\n",
        "    pred = val_preds[vidx]\n",
        "    conf = max_confs[vidx]\n",
        "    color = 'green' if pred == true_label else 'red'\n",
        "    ax.set_title(f'True: {CLASS_NAMES[true_label]}\\nPred: {CLASS_NAMES[pred]} ({conf:.0%})',\n",
        "                 fontsize=8, color=color, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Least Confident Predictions (Ambiguous Cases)',\n",
        "             fontsize=13, fontweight='bold', color='orange')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jttmHi0voNcY"
      },
      "source": [
        "---\n",
        "# PART F — Test Inference & Submission\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0FV39C2ToNcY"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_with_tta(image_path, model, tta_transforms, device, num_classes=10):\n",
        "    model.eval()\n",
        "    img_pil = Image.open(image_path).convert('RGB')\n",
        "    avg_probs = torch.zeros(num_classes).to(device)\n",
        "    for t in tta_transforms:\n",
        "        inp = t(img_pil).unsqueeze(0).to(device)\n",
        "        with autocast(enabled=(device == 'cuda')):\n",
        "            logits = model(inp)\n",
        "        avg_probs += F.softmax(logits, dim=1).squeeze(0)\n",
        "    avg_probs /= len(tta_transforms)\n",
        "    return avg_probs.cpu().numpy()\n",
        "\n",
        "\n",
        "def predict_test_folder(folder_path, model, tta_transforms, device, use_tta=True):\n",
        "    model.eval()\n",
        "    names, probs = [], []\n",
        "    files = sorted(glob.glob(folder_path + '/*'))\n",
        "    print(f\"Predicting {len(files)} images (TTA={'ON' if use_tta else 'OFF'})...\")\n",
        "\n",
        "    for fpath in tqdm(files, desc='Inference'):\n",
        "        if use_tta:\n",
        "            p = predict_with_tta(fpath, model, tta_transforms, device)\n",
        "        else:\n",
        "            img = Image.open(fpath).convert('RGB')\n",
        "            inp = val_transform(img).unsqueeze(0).to(device)\n",
        "            with torch.no_grad(), autocast(enabled=(device == 'cuda')):\n",
        "                logits = model(inp)\n",
        "            p = F.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
        "        names.append(os.path.basename(fpath))\n",
        "        probs.append(p)\n",
        "\n",
        "    df = pd.DataFrame(probs, columns=[f'c{i}' for i in range(10)])\n",
        "    df.insert(0, 'img', names)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0yotPaPjoNcY"
      },
      "outputs": [],
      "source": [
        "submission_df = predict_test_folder(TEST_PATH, trained_model, tta_transforms, DEVICE, use_tta=True)\n",
        "print(f\"\\nSubmission shape: {submission_df.shape}\")\n",
        "print(f\"Nulls: {submission_df.isnull().sum().sum()}\")\n",
        "submission_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bJ6qZ7TCoNcY"
      },
      "outputs": [],
      "source": [
        "# Confidence analysis on test set\n",
        "test_max_probs = submission_df[[f'c{i}' for i in range(10)]].max(axis=1)\n",
        "test_pred_classes = submission_df[[f'c{i}' for i in range(10)]].idxmax(axis=1)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "axes[0].hist(test_max_probs, bins=50, color='steelblue', edgecolor='white', alpha=0.8)\n",
        "axes[0].axvline(test_max_probs.median(), color='red', ls='--',\n",
        "                label=f'Median: {test_max_probs.median():.3f}')\n",
        "axes[0].set_title('Test Prediction Confidence', fontweight='bold')\n",
        "axes[0].set_xlabel('Max Probability'); axes[0].set_ylabel('Count'); axes[0].legend()\n",
        "\n",
        "pred_counts = test_pred_classes.value_counts().sort_index()\n",
        "axes[1].bar(pred_counts.index, pred_counts.values,\n",
        "            color=plt.cm.Set3(np.linspace(0, 1, 10)), edgecolor='gray')\n",
        "axes[1].set_title('Predicted Class Distribution (Test)', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "low_conf = (test_max_probs < 0.5).sum()\n",
        "print(f\"Low confidence (<50%): {low_conf}/{len(test_max_probs)} ({100*low_conf/len(test_max_probs):.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jcb4NPfnoNcY"
      },
      "outputs": [],
      "source": [
        "# Save submission\n",
        "sub_path = '/content/drive/MyDrive/DS project/submission_audit.csv'\n",
        "submission_df.to_csv(sub_path, index=False)\n",
        "print(f\"Saved to {sub_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP3U1oczoNcZ"
      },
      "source": [
        "---\n",
        "# PART G — Summary & Recommendations\n",
        "*Covers: §5 — Summary*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pZrPx4_JoNcZ"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# G.1 — Comprehensive Audit Summary\n",
        "# ====================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"AUDIT SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "§5a — Was the data appropriate for this ADS?\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "STRENGTHS:\n",
        "- Reasonably balanced across 10 classes\n",
        "- Consistent image dimensions and format (RGB, 640×480)\n",
        "- Real-world driving scenarios (not synthetic)\n",
        "\n",
        "WEAKNESSES:\n",
        "- Limited driver diversity (~26 unique drivers): model may learn\n",
        "  driver identity rather than distraction behavior\n",
        "- No demographic metadata: cannot audit for racial, gender, or age bias\n",
        "- Controlled environment (consistent vehicle interior): may not\n",
        "  generalize to diverse vehicle types and lighting conditions\n",
        "- Some classes are visually ambiguous (e.g., texting vs operating radio)\n",
        "\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "§5b — Is the implementation robust, accurate, and fair?\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "\"\"\")\n",
        "\n",
        "print(f\"Accuracy:  {val_acc:.2f}%\")\n",
        "print(f\"Macro F1:  {f1_score(val_labels, val_preds, average='macro'):.4f}\")\n",
        "print(f\"LogLoss:   {val_logloss:.4f}\")\n",
        "print(f\"ECE:       {ece:.4f}\")\n",
        "print(f\"TPR gap:   {tpr_gap:.4f}\")\n",
        "print(f\"FPR gap:   {fpr_gap:.4f}\")\n",
        "\n",
        "print(\"\"\"\n",
        "ACCURACY: The EfficientNet-B3 model achieves strong per-class accuracy.\n",
        "  However, accuracy varies across classes — the model struggles most\n",
        "  with visually similar distraction types.\n",
        "\n",
        "ROBUSTNESS: Performance degrades under noise and blur, which are\n",
        "  realistic in-vehicle conditions. The model needs additional\n",
        "  hardening for production deployment.\n",
        "\n",
        "FAIRNESS: Without demographic labels, we used per-class and per-driver\n",
        "  accuracy as proxy fairness metrics. The TPR/FPR gaps indicate\n",
        "  some classes are systematically harder, raising equalized-odds concerns.\n",
        "\n",
        "CALIBRATION: The model's confidence scores are [see ECE above].\n",
        "  Well-calibrated confidence is critical for a safety system.\n",
        "\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "§5c — Would we deploy this ADS?\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "NOT YET. Before deployment, we would need:\n",
        "\n",
        "1. Diverse training data spanning many drivers, vehicle types,\n",
        "   lighting conditions, and demographics\n",
        "2. Driver-aware evaluation (not random splits) to ensure\n",
        "   generalization to unseen individuals\n",
        "3. Robustness hardening against common corruptions\n",
        "4. A selective prediction mechanism (reject uncertain predictions\n",
        "   and defer to human review)\n",
        "5. Demographic fairness audit with labeled subgroup data\n",
        "6. Real-time performance benchmarking on edge devices\n",
        "\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "§5d — Recommended improvements\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "DATA:\n",
        "- Collect data from >100 diverse drivers\n",
        "- Include demographic metadata for fairness auditing\n",
        "- Add nighttime and varied weather conditions\n",
        "- Include borderline/ambiguous cases with multi-annotator labels\n",
        "\n",
        "MODEL:\n",
        "- Use driver-aware cross-validation\n",
        "- Apply adversarial training for robustness\n",
        "- Add a \"reject\" option for low-confidence predictions\n",
        "- Use Focal Loss to emphasize hard examples\n",
        "- Ensemble multiple architectures\n",
        "\n",
        "DEPLOYMENT:\n",
        "- Implement sliding-window temporal smoothing (multiple frames)\n",
        "- Set asymmetric thresholds: prioritize detecting distraction\n",
        "  (minimize false negatives) even at cost of more false alarms\n",
        "- Regular model re-training as new data becomes available\n",
        "- Human-in-the-loop review for edge cases\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DGoCBxLdoNcZ"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# G.2 — Audit Metrics Dashboard (Summary Table)\n",
        "# ====================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"AUDIT METRICS DASHBOARD\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "dashboard = pd.DataFrame([\n",
        "    ['Overall Accuracy', f'{val_acc:.2f}%', '>95%', '✓' if val_acc > 95 else '✗'],\n",
        "    ['Macro F1 Score', f'{f1_score(val_labels, val_preds, average=\"macro\"):.4f}', '>0.95', '✓' if f1_score(val_labels, val_preds, average=\"macro\") > 0.95 else '✗'],\n",
        "    ['LogLoss (Kaggle)', f'{val_logloss:.4f}', '<0.5', '✓' if val_logloss < 0.5 else '✗'],\n",
        "    ['ECE (Calibration)', f'{ece:.4f}', '<0.05', '✓' if ece < 0.05 else '✗'],\n",
        "    ['TPR Gap (Eq. Odds)', f'{tpr_gap:.4f}', '<0.05', '✓' if tpr_gap < 0.05 else '✗'],\n",
        "    ['FPR Gap (Eq. Odds)', f'{fpr_gap:.4f}', '<0.02', '✓' if fpr_gap < 0.02 else '✗'],\n",
        "    ['Accuracy Gap (max-min class)', f'{acc_gap*100:.2f}%', '<5%', '✓' if acc_gap < 0.05 else '✗'],\n",
        "    ['Macro AUC', f'{macro_auc:.4f}', '>0.99', '✓' if macro_auc > 0.99 else '✗'],\n",
        "], columns=['Metric', 'Value', 'Target', 'Pass'])\n",
        "\n",
        "print(dashboard.to_string(index=False))\n",
        "\n",
        "passed = (dashboard['Pass'] == '✓').sum()\n",
        "total = len(dashboard)\n",
        "print(f\"\\nPassed: {passed}/{total} metrics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EjhGtPJJoNcZ"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"NOTEBOOK COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "This notebook covers all required sections:\n",
        "  §1 Background                          ✓ (Part A)\n",
        "  §2 Input and Output                    ✓ (Part A — data profiling)\n",
        "  §3 Implementation and Validation       ✓ (Part B — model + training)\n",
        "  §4a Accuracy across subpopulations     ✓ (Part C)\n",
        "  §4b Fairness analysis                  ✓ (Part D)\n",
        "  §4c Stability, robustness, interpret.  ✓ (Part E)\n",
        "  §5 Summary & recommendations           ✓ (Part G)\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}